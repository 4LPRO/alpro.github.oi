{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4LPRO/alpro.github.oi/blob/main/tutorials/speaker_tasks/SIV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyLoWDsb9rEs",
        "outputId": "da79e632-08f0-4d5a-91c7-355f9774ffb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=6597441984d6af6d697be4f3ad773870bc45b784c8d6f96cc139e88ed3f1d498\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base libsox3 libwavpack1\n",
            "Suggested packages:\n",
            "  libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base libsox3 libwavpack1 sox\n",
            "0 upgraded, 7 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 617 kB of archives.\n",
            "After this operation, 1,764 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [240 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [11.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwavpack1 amd64 5.4.0-1build2 [83.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [33.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 sox amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [104 kB]\n",
            "Fetched 617 kB in 2s (276 kB/s)\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../2-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../3-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../4-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../5-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../6-sox_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (1.3)\n",
            "\u001b[33mDEPRECATION: git+https://github.com/NVIDIA/NeMo.git@r2.0.0rc0#egg=nemo_toolkit[asr] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting nemo_toolkit (from nemo_toolkit[asr])\n",
            "  Cloning https://github.com/NVIDIA/NeMo.git (to revision r2.0.0rc0) to /tmp/pip-install-08mgl_ic/nemo-toolkit_811d19793e064589a4c0d814a8841353\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/NeMo.git /tmp/pip-install-08mgl_ic/nemo-toolkit_811d19793e064589a4c0d814a8841353\n",
            "  Running command git checkout -b r2.0.0rc0 --track origin/r2.0.0rc0\n",
            "  Switched to a new branch 'r2.0.0rc0'\n",
            "  Branch 'r2.0.0rc0' set up to track remote branch 'r2.0.0rc0' from 'origin'.\n",
            "  Resolved https://github.com/NVIDIA/NeMo.git to commit d02bb32a36eecab15e2782839eb5b6838df5cb88\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fiddle (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading fiddle-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: huggingface_hub>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (0.28.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (0.61.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (1.26.4)\n",
            "Collecting onnx>=1.7.0 (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (2.8.2)\n",
            "Collecting ruamel.yaml (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (75.1.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (2.18.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (1.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (4.67.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (3.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (1.17.2)\n",
            "Collecting braceexpand (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (0.8.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (0.8.1)\n",
            "Collecting g2p_en (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (7.7.1)\n",
            "Collecting jiwer (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting kaldi-python-io (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading kaldi-python-io-1.2.2.tar.gz (8.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kaldiio (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading kaldiio-2.18.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lhotse>=1.22.0 (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading lhotse-1.29.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (0.10.2.post1)\n",
            "Collecting marshmallow (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (24.2)\n",
            "Collecting pyannote.core (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.metrics (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pydub (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyloudnorm (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting resampy (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (1.13.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (0.13.1)\n",
            "Collecting sox (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading sox-1.5.0.tar.gz (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting texterrors (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading texterrors-1.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (3.1.1)\n",
            "Collecting hydra-core<=1.3.2,>1.3 (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting omegaconf<=2.3 (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pytorch-lightning>=2.2.1 (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torchmetrics>=0.11.0 (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading torchmetrics-1.6.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting transformers<=4.40.2,>=4.36.0 (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (0.19.7)\n",
            "Collecting webdataset>=0.2.86 (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting datasets (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (7.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (2.2.2)\n",
            "Collecting sacremoses>=0.0.43 (from nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: sentencepiece<1.0.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit->nemo_toolkit[asr]) (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.20.3->nemo_toolkit->nemo_toolkit[asr]) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.20.3->nemo_toolkit->nemo_toolkit[asr]) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.20.3->nemo_toolkit->nemo_toolkit[asr]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.20.3->nemo_toolkit->nemo_toolkit[asr]) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.20.3->nemo_toolkit->nemo_toolkit[asr]) (4.12.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core<=1.3.2,>1.3->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[asr]) (3.0.1)\n",
            "Requirement already satisfied: click>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[asr]) (8.1.8)\n",
            "Collecting cytoolz>=0.10.1 (from lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting intervaltree>=3.1.0 (from lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[asr]) (0.9.0)\n",
            "Collecting lilcom>=1.1.0 (from lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading lilcom-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[asr]) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[asr]) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[asr]) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[asr]) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[asr]) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[asr]) (1.1.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->nemo_toolkit->nemo_toolkit[asr]) (0.44.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.7.0->nemo_toolkit->nemo_toolkit[asr]) (4.25.6)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=2.2.1->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading lightning_utilities-0.13.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses>=0.0.43->nemo_toolkit->nemo_toolkit[asr]) (2024.11.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nemo_toolkit->nemo_toolkit[asr]) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile->nemo_toolkit->nemo_toolkit[asr]) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[asr]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[asr]) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[asr]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[asr]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[asr]) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit->nemo_toolkit[asr]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->nemo_toolkit->nemo_toolkit[asr]) (1.3.0)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers<=4.40.2,>=4.36.0->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.40.2,>=4.36.0->nemo_toolkit->nemo_toolkit[asr]) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->nemo_toolkit->nemo_toolkit[asr]) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->nemo_toolkit->nemo_toolkit[asr]) (3.11.13)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from fiddle->nemo_toolkit->nemo_toolkit[asr]) (1.4.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from fiddle->nemo_toolkit->nemo_toolkit[asr]) (0.20.3)\n",
            "Collecting libcst (from fiddle->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading libcst-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.11/dist-packages (from g2p_en->nemo_toolkit->nemo_toolkit[asr]) (3.9.1)\n",
            "Collecting distance>=0.1.3 (from g2p_en->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect->nemo_toolkit->nemo_toolkit[asr]) (10.6.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect->nemo_toolkit->nemo_toolkit[asr]) (4.4.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (3.0.13)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit->nemo_toolkit[asr]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit->nemo_toolkit[asr]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit->nemo_toolkit[asr]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit->nemo_toolkit[asr]) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit->nemo_toolkit[asr]) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit->nemo_toolkit[asr]) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->nemo_toolkit->nemo_toolkit[asr]) (1.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->nemo_toolkit->nemo_toolkit[asr]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nemo_toolkit->nemo_toolkit[asr]) (2025.1)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core->nemo_toolkit->nemo_toolkit[asr]) (2.4.0)\n",
            "Collecting pyannote.database>=4.0.1 (from pyannote.metrics->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from pyloudnorm->nemo_toolkit->nemo_toolkit[asr]) (1.0.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->nemo_toolkit->nemo_toolkit[asr]) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->nemo_toolkit->nemo_toolkit[asr]) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->nemo_toolkit->nemo_toolkit[asr]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->nemo_toolkit->nemo_toolkit[asr]) (3.1.3)\n",
            "Collecting pybind11 (from texterrors->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting plac (from texterrors->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading plac-1.4.3-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting loguru (from texterrors->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from texterrors->nemo_toolkit->nemo_toolkit[asr]) (2.5.0)\n",
            "Collecting Levenshtein (from texterrors->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[asr]) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[asr]) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[asr]) (4.3.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[asr]) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[asr]) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[asr]) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit->nemo_toolkit[asr]) (1.3.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile->nemo_toolkit->nemo_toolkit[asr]) (2.22)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from cytoolz>=0.10.1->lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[asr]) (0.12.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[asr]) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[asr]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[asr]) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[asr]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[asr]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[asr]) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[asr]) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit->nemo_toolkit[asr]) (4.0.12)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (1.6.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (6.4.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr])\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (4.9.0)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit->nemo_toolkit[asr]) (0.15.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->nemo_toolkit->nemo_toolkit[asr]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->nemo_toolkit->nemo_toolkit[asr]) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.20.3->nemo_toolkit->nemo_toolkit[asr]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.20.3->nemo_toolkit->nemo_toolkit[asr]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.20.3->nemo_toolkit->nemo_toolkit[asr]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.20.3->nemo_toolkit->nemo_toolkit[asr]) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->nemo_toolkit->nemo_toolkit[asr]) (3.0.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (6.5.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit->nemo_toolkit[asr]) (5.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (5.7.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (1.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.2.13)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit->nemo_toolkit[asr]) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit->nemo_toolkit[asr]) (13.9.4)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (4.23.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit->nemo_toolkit[asr]) (3.0.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (1.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (0.23.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit->nemo_toolkit[asr]) (0.1.2)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (1.24.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (2.6)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[asr]) (1.3.1)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lhotse-1.29.0-py3-none-any.whl (843 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m843.9/843.9 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.2-py3-none-any.whl (931 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdataset-0.2.111-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiddle-0.3.0-py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/419.8 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texterrors-1.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.13.1-py3-none-any.whl (28 kB)\n",
            "Downloading lilcom-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libcst-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plac-1.4.3-py2.py3-none-any.whl (22 kB)\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nemo_toolkit, antlr4-python3-runtime, kaldi-python-io, sox, distance, docopt, intervaltree\n",
            "  Building wheel for nemo_toolkit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nemo_toolkit: filename=nemo_toolkit-2.0.0rc0-py3-none-any.whl size=3638576 sha256=4b8af33c25681845fae21d1ecfd65f946bbb34363117e2873558b9acf8b5b6df\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ln4wsgf8/wheels/ec/1f/67/450cab4752030c1a8505b9677a5ec44490c78e5b39305b419d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=4e325613e19543d317c9d2dba61830558211abcd21150ee12a4cbcce487c7cac\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaldi-python-io: filename=kaldi_python_io-1.2.2-py3-none-any.whl size=8952 sha256=9520f082cd8db16f597bbadd53faf685d08c7392a54f046705c4087bad42b5eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/86/7b/eec1bb7dc63b8aab5da6317609313873e6e75f065b65f3c29c\n",
            "  Building wheel for sox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sox: filename=sox-1.5.0-py3-none-any.whl size=40037 sha256=069131ad8b53ee6b3e173b825443206f934146db83537694907f5a25a4c16d6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/89/93/023fcdacaec4e5471e78b43992515e8500cc2505b307e2e6b7\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16256 sha256=b971c29047deb8cf3da4e1de74e737775d843f06cf3c71c475430538f047056d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/cd/9c/3ab5d666e3bcacc58900b10959edd3816cc9557c7337986322\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=01542fcdf7a0e71a52e0d94d724c6615b7bb100360bfb4320aa97426f97ef6cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=c16a24a2a98cd27aaf878c5d368a8a0579426899ff8288189dcca2ae922ea013\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n",
            "Successfully built nemo_toolkit antlr4-python3-runtime kaldi-python-io sox distance docopt intervaltree\n",
            "Installing collected packages: pydub, plac, docopt, distance, braceexpand, antlr4-python3-runtime, xxhash, webdataset, sox, sacremoses, ruamel.yaml.clib, rapidfuzz, pybind11, onnx, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, marshmallow, loguru, lilcom, lightning-utilities, libcst, kaldiio, kaldi-python-io, jedi, intervaltree, dill, cytoolz, ruamel.yaml, resampy, pyloudnorm, pyannote.core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, Levenshtein, jiwer, hydra-core, fiddle, tokenizers, texterrors, nvidia-cusolver-cu12, g2p_en, transformers, pyannote.database, datasets, torchmetrics, pyannote.metrics, nemo_toolkit, lhotse, pytorch-lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.27.1 antlr4-python3-runtime-4.9.3 braceexpand-0.1.7 cytoolz-1.0.1 datasets-3.3.2 dill-0.3.8 distance-0.1.3 docopt-0.6.2 fiddle-0.3.0 g2p_en-2.1.0 hydra-core-1.3.2 intervaltree-3.1.0 jedi-0.19.2 jiwer-3.1.0 kaldi-python-io-1.2.2 kaldiio-2.18.0 lhotse-1.29.0 libcst-1.6.0 lightning-utilities-0.13.1 lilcom-1.8.0 loguru-0.7.3 marshmallow-3.26.1 multiprocess-0.70.16 nemo_toolkit-2.0.0rc0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0 onnx-1.17.0 plac-1.4.3 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pybind11-2.13.6 pydub-0.25.1 pyloudnorm-0.1.1 pytorch-lightning-2.5.0.post0 rapidfuzz-3.12.2 resampy-0.4.3 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 sacremoses-0.1.1 sox-1.5.0 texterrors-1.0.7 tokenizers-0.19.1 torchmetrics-1.6.2 transformers-4.40.2 webdataset-0.2.111 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell.\n",
        "\n",
        "# Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install text-unidecode\n",
        "\n",
        "## Install NeMo\n",
        "BRANCH = 'r2.0.0rc0'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
        "\n",
        "# Install TorchAudio\n",
        "!pip install torchaudio>=0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDzak_FIB9LS"
      },
      "source": [
        "# **SPEAKER RECOGNITION**\n",
        "Speaker Recognition (SR) is a broad research area that solves two major tasks: speaker identification (who is speaking?) and\n",
        "speaker verification (is the speaker who they claim to be?). In this work, we focus on text-independent speaker recognition when the identity of the speaker is based on how the speech is spoken,\n",
        "not necessarily in what is being said. Typically such SR systems operate on unconstrained speech utterances,\n",
        "which are converted into fixed-length vectors, called speaker embeddings. Speaker embeddings are also used in\n",
        "automatic speech recognition (ASR) and speech synthesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydqmdcDxCeXb"
      },
      "source": [
        "In this tutorial, we shall first train these embeddings on speaker-related datasets, and then get speaker embeddings from a pretrained network for a new dataset. Since Google Colab has very slow read-write speeds, I'll be demonstrating this tutorial using [an4](http://www.speech.cs.cmu.edu/databases/an4/).\n",
        "\n",
        "Instead, if you'd like to try on a bigger dataset like [hi-mia](https://arxiv.org/abs/1912.01231) use the [get_hi-mia-data.py](https://github.com/NVIDIA/NeMo/tree/main/scripts/dataset_processing/speaker_tasks/get_hi-mia_data.py) script to download the necessary files, extract them, and resample to 16Khz if any of these samples are not at 16Khz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqUBayc_Ctcr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "NEMO_ROOT = os.getcwd()\n",
        "print(NEMO_ROOT)\n",
        "import glob\n",
        "import subprocess\n",
        "import tarfile\n",
        "import wget\n",
        "\n",
        "data_dir = os.path.join(NEMO_ROOT,'data')\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Download the dataset. This will take a few moments...\n",
        "print(\"******\")\n",
        "if not os.path.exists(data_dir + '/an4_sphere.tar.gz'):\n",
        "    an4_url = 'https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz'\n",
        "    an4_path = wget.download(an4_url, data_dir)\n",
        "    print(f\"Dataset downloaded at: {an4_path}\")\n",
        "else:\n",
        "    print(\"Tarfile already exists.\")\n",
        "    an4_path = data_dir + '/an4_sphere.tar.gz'\n",
        "\n",
        "# Untar and convert .sph to .wav (using sox)\n",
        "tar = tarfile.open(an4_path)\n",
        "tar.extractall(path=data_dir)\n",
        "\n",
        "print(\"Converting .sph to .wav...\")\n",
        "sph_list = glob.glob(data_dir + '/an4/**/*.sph', recursive=True)\n",
        "for sph_path in sph_list:\n",
        "    wav_path = sph_path[:-4] + '.wav'\n",
        "    cmd = [\"sox\", sph_path, wav_path]\n",
        "    subprocess.run(cmd)\n",
        "print(\"Finished conversion.\\n******\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5PrWzkiDbHy"
      },
      "source": [
        "Since an4 is not designed for speaker recognition, this facilitates the opportunity to demonstrate how you can generate manifest files that are necessary for training. These methods can be applied to any dataset to get similar training manifest files.\n",
        "\n",
        "First, create a list file which has all the wav files with absolute paths for each of the train, dev, and test set. This can be easily done by the `find` bash command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnrUh3vuDSRN"
      },
      "outputs": [],
      "source": [
        "!find {data_dir}/an4/wav/an4_clstk  -iname \"*.wav\" > data/an4/wav/an4_clstk/train_all.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhWVg2QoDhL3"
      },
      "source": [
        "Let's look at the first 3 lines of filelist text file for train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfnMK302Du20"
      },
      "outputs": [],
      "source": [
        "!head -n 3 {data_dir}/an4/wav/an4_clstk/train_all.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9L9Tl0XDw5Z"
      },
      "source": [
        "Since we created the list text file for the train, we use `filelist_to_manifest.py` to convert this text file to a manifest file and then optionally split the files to train \\& dev for evaluating the models during training by using the `--split` flag. We wouldn't be needing the `--split` option for the test folder.\n",
        "Accordingly please mention the `id` number, which is the field num separated by `/` to be considered as the speaker label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LYwHAr1G8hp"
      },
      "source": [
        "After the download and conversion, your `data` folder should contain directories with manifest files as:\n",
        "\n",
        "* `data/<path>/train.json`\n",
        "* `data/<path>/dev.json`\n",
        "* `data/<path>/train_all.json`\n",
        "\n",
        "Each line in the manifest file describes a training sample - `audio_filepath` contains the path to the wav file, `duration` it's duration in seconds, and `label` is the speaker class label:\n",
        "\n",
        "`{\"audio_filepath\": \"<absolute path to dataset>data/an4/wav/an4test_clstk/menk/cen4-menk-b.wav\", \"duration\": 3.9, \"label\": \"menk\"}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpAv77JoD98c"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('scripts'):\n",
        "  print(\"Downloading necessary scripts\")\n",
        "  !mkdir -p scripts/speaker_tasks\n",
        "  !wget -P scripts/speaker_tasks/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/speaker_tasks/filelist_to_manifest.py\n",
        "!python {NEMO_ROOT}/scripts/speaker_tasks/filelist_to_manifest.py --filelist {data_dir}/an4/wav/an4_clstk/train_all.txt --id -2 --out {data_dir}/an4/wav/an4_clstk/all_manifest.json --split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kPCmx5DHvY5"
      },
      "source": [
        "Generate the list text file for the test folder and then convert it to a manifest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMd24GVaFBwr"
      },
      "outputs": [],
      "source": [
        "!find {data_dir}/an4/wav/an4test_clstk  -iname \"*.wav\" > {data_dir}/an4/wav/an4test_clstk/test_all.txt\n",
        "!python {NEMO_ROOT}/scripts/speaker_tasks/filelist_to_manifest.py --filelist {data_dir}/an4/wav/an4test_clstk/test_all.txt --id -2 --out {data_dir}/an4/wav/an4test_clstk/test.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5FPmxUkGakD"
      },
      "source": [
        "## Path to manifest files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo-VnYPtJO_v"
      },
      "outputs": [],
      "source": [
        "train_manifest = os.path.join(data_dir,'an4/wav/an4_clstk/train.json')\n",
        "validation_manifest = os.path.join(data_dir,'an4/wav/an4_clstk/dev.json')\n",
        "test_manifest = os.path.join(data_dir,'an4/wav/an4_clstk/dev.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyDVdtjAL2__"
      },
      "source": [
        "As the goal of most speaker-related systems is to get good speaker level embeddings that could help distinguish from\n",
        "other speakers, we shall first train these embeddings in an end-to-end\n",
        "manner optimizing the [TitaNet](https://arxiv.org/pdf/2110.04410.pdf) model.\n",
        "We modify the decoder to get these fixed-size embeddings irrespective of the length of the input audio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJtU_GEdMUUo"
      },
      "source": [
        "# Training\n",
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH80lDWdL0BN"
      },
      "source": [
        "Note: All the following steps are just for explanation of each section, but one can use the provided [training script](https://github.com/NVIDIA/NeMo/blob/main/examples/speaker_tasks/recognition/speaker_reco.py) to launch training in the command line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1ojB0cZMSmv"
      },
      "outputs": [],
      "source": [
        "import nemo\n",
        "# NeMo's ASR collection - This collection contains complete ASR models and\n",
        "# building blocks (modules) for ASR\n",
        "import nemo.collections.asr as nemo_asr\n",
        "from omegaconf import OmegaConf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Zho11LNAFJ"
      },
      "source": [
        "## Model Configuration\n",
        "The TitaNet model is defined in a config file which declares multiple important sections.\n",
        "\n",
        "They are:\n",
        "\n",
        "1) model: All arguments that will relate to the Model - preprocessors, encoder, decoder, optimizer and schedulers, datasets, and any other related information\n",
        "\n",
        "2) trainer: Any argument to be passed to PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HQtZfKnMhpI"
      },
      "outputs": [],
      "source": [
        "# This line will print the entire config of sample TitaNet model\n",
        "!mkdir conf\n",
        "!wget -P conf https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/speaker_tasks/recognition/conf/titanet-large.yaml\n",
        "MODEL_CONFIG = os.path.join(NEMO_ROOT,'conf/titanet-large.yaml')\n",
        "config = OmegaConf.load(MODEL_CONFIG)\n",
        "print(OmegaConf.to_yaml(config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtbXN-cFOwxi"
      },
      "source": [
        "## Setting up the datasets within the config\n",
        "If you'll notice, there are a few config dictionaries called train_ds, validation_ds and test_ds. These are configurations used to setup the Dataset and DataLoaders of the corresponding config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPBIf1jmNgjn"
      },
      "outputs": [],
      "source": [
        "print(OmegaConf.to_yaml(config.model.train_ds))\n",
        "print(OmegaConf.to_yaml(config.model.validation_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLIjKOMUP0YE"
      },
      "source": [
        "You will often notice that some configs have ??? in place of paths. This is used as a placeholder so that the user can change the value at a later time.\n",
        "\n",
        "Let's add the paths to the manifests to the config above\n",
        "Also, since an4 dataset doesn't have a test set of the same speakers used in training, we will use validation manifest as test manifest for demonstration purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSotpjL_O2BN"
      },
      "outputs": [],
      "source": [
        "config.model.train_ds.manifest_filepath = train_manifest\n",
        "config.model.validation_ds.manifest_filepath = validation_manifest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DEO71TxL0BO"
      },
      "source": [
        "Note: Since we are training speaker embedding extractor model for verification we do not add test_ds dataset. To include it add it to config and replace manifest file as\n",
        "`config.model.test_ds.manifest_filepath = test_manifest`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy6_Lf6fW9aJ"
      },
      "source": [
        "Also as we are training on an4 dataset, there are 74 speaker labels in training, and we need to set this in the decoder config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B96tFTnW8Yh"
      },
      "outputs": [],
      "source": [
        "config.model.decoder.num_classes = 74"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83pHBRDpQTF0"
      },
      "source": [
        "## Building the PyTorch Lightning Trainer\n",
        "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem!\n",
        "\n",
        "Let us first instantiate a Trainer object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWzGJoHMQQnG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIYf4-KFQYHl"
      },
      "outputs": [],
      "source": [
        "print(\"Trainer config - \\n\")\n",
        "print(OmegaConf.to_yaml(config.trainer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXuSMYMNQeW7"
      },
      "outputs": [],
      "source": [
        "# Let us modify some trainer configs for this demo\n",
        "# Checks if we have GPU available and uses it\n",
        "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "config.trainer.devices = 1\n",
        "config.trainer.accelerator = accelerator\n",
        "\n",
        "# Reduces maximum number of epochs to 5 for quick demonstration\n",
        "config.trainer.max_epochs = 10\n",
        "\n",
        "# Remove distributed training flags\n",
        "config.trainer.strategy = 'auto'\n",
        "\n",
        "# Remove augmentations\n",
        "config.model.train_ds.augmentor=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBq3eCLwQhCy"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(**config.trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xHq_rcmQiry"
      },
      "source": [
        "## Setting up a NeMo Experiment\n",
        "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMm8MPYfQsCS"
      },
      "outputs": [],
      "source": [
        "from nemo.utils.exp_manager import exp_manager\n",
        "log_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
        "# The log_dir provides a path to the current logging directory for easy access\n",
        "print(log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQQMlXmLQ7h1"
      },
      "source": [
        "## Building the TitaNet Model\n",
        "TitaNet is a speaker embedding extractor model that can be used for speaker identification tasks - it generates one label for the entire provided audio stream. Therefore we encapsulate it inside the EncDecSpeakerLabelModel as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_KY_s5LROYf"
      },
      "outputs": [],
      "source": [
        "speaker_model = nemo_asr.models.EncDecSpeakerLabelModel(cfg=config.model, trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AphpMhkSVdU"
      },
      "source": [
        "Before we begin training, let us first create a Tensorboard visualization to monitor progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUnDpe_5SbDR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google import colab\n",
        "  COLAB_ENV = True\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "  COLAB_ENV = False\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "if COLAB_ENV:\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir {exp_dir}\n",
        "else:\n",
        "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or8g1cksSf8C"
      },
      "source": [
        "As any NeMo model is inherently a PyTorch Lightning Model, it can easily be trained in a single line - trainer.fit(model)!\n",
        "Below we see that the model begins to get modest scores on the validation set after just 5 epochs of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvYhsOWuSpL_",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "trainer.fit(speaker_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSRACGt3UAYn"
      },
      "source": [
        "This config is not suited and designed for an4 so you may observe unstable val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvtVKO8FZsoe"
      },
      "source": [
        "If you have a test manifest file, we can easily compute test accuracy by running\n",
        "<pre><code>trainer.test(speaker_model, ckpt_path=None)\n",
        "</code></pre>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlBwMsRdZfqg"
      },
      "source": [
        "## For Faster Training\n",
        "We can dramatically improve the time taken to train this model by using Multi GPU training along with Mixed Precision.\n",
        "\n",
        "### Trainer with a distributed backend:\n",
        "<pre><code>trainer = Trainer(devices=2, num_nodes=2, accelerator='gpu', strategy='auto')\n",
        "</code></pre>\n",
        "\n",
        "### Mixed precision:\n",
        "<pre><code>trainer = Trainer(amp_level='O1', precision=16)\n",
        "</code></pre>\n",
        "\n",
        "Of course, you can combine these flags as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcnWub9-0TW2"
      },
      "source": [
        "## Saving/Restoring a checkpoint\n",
        "There are multiple ways to save and load models in NeMo. Since all NeMo models are inherently Lightning Modules, we can use the standard way that PyTorch Lightning saves and restores models.\n",
        "\n",
        "NeMo also provides a more advanced model save/restore format, which encapsulates all the parts of the model that are required to restore that model for immediate use.\n",
        "\n",
        "In this example, we will explore both ways of saving and restoring models, but we will focus on the PyTorch Lightning method.\n",
        "\n",
        "## Saving and Restoring via PyTorch Lightning Checkpoints\n",
        "When using NeMo for training, it is advisable to utilize the exp_manager framework. It is tasked with handling checkpointing and logging (Tensorboard as well as WandB optionally!), as well as dealing with multi-node and multi-GPU logging.\n",
        "\n",
        "Since we utilized the exp_manager framework above, we have access to the directory where the checkpoints exist.\n",
        "\n",
        "exp_manager with the default settings will save multiple checkpoints for us -\n",
        "\n",
        "1) A few checkpoints from certain steps of training. They will have --val_loss= tags\n",
        "\n",
        "2) Checkpoints at the last epoch of training are denoted by --last.\n",
        "\n",
        "3) If the model finishes training, it will also have a --last checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSLjq-edaPt_"
      },
      "outputs": [],
      "source": [
        "# Let us list all the checkpoints we have\n",
        "checkpoint_dir = os.path.join(log_dir, 'checkpoints')\n",
        "checkpoint_paths = list(glob.glob(os.path.join(checkpoint_dir, \"*.ckpt\")))\n",
        "checkpoint_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwltdVWXaroa"
      },
      "outputs": [],
      "source": [
        "final_checkpoint = list(filter(lambda x: \"-last.ckpt\" in x, checkpoint_paths))[0]\n",
        "print(final_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tGKKojs0fEh"
      },
      "source": [
        "\n",
        "## Restoring from a PyTorch Lightning checkpoint\n",
        "To restore a model using the LightningModule.load_from_checkpoint() class method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgyP9cYVbFc8"
      },
      "outputs": [],
      "source": [
        "restored_model = nemo_asr.models.EncDecSpeakerLabelModel.load_from_checkpoint(final_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnZVMKZpbI_M"
      },
      "source": [
        "# Finetuning\n",
        "Since we don't have any new manifest file to finetune, I will demonstrate here by using the test manifest file we created earlier.\n",
        "an4 test dataset has a different set of speakers from the train set (total number: 10). And as we didn't split this dataset for validation I will use the same for validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV9gInFwQ2F5"
      },
      "source": [
        "There are a couple of ways we can finetune a speaker recognition model.\n",
        "1. Finetuning using a pretrained model published on NGC.\n",
        "2. Finetuning from a PTL checkpoint.\n",
        "\n",
        "Since finetuning from a large pretrained model is more common, I shall use it to demonstrate finetuning procedure. In order to make finetuning step independent from training from scratch, we use another config. Here we shall use `titanet-finetune.yaml` config, that is created to show finetuning on pretrained titanet-large model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tHyT5gXL0BU"
      },
      "source": [
        "Note: You may use [finetune-script](https://github.com/NVIDIA/NeMo/blob/main/examples/speaker_tasks/recognition/speaker_reco_finetune.py) to launch training in the command line. Following is just a demonstration of the script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2bMXNDTL0BU"
      },
      "outputs": [],
      "source": [
        "!wget -P conf https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/speaker_tasks/recognition/conf/titanet-finetune.yaml\n",
        "MODEL_CONFIG = os.path.join(NEMO_ROOT,'conf/titanet-finetune.yaml')\n",
        "finetune_config = OmegaConf.load(MODEL_CONFIG)\n",
        "print(OmegaConf.to_yaml(finetune_config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWLAc9-7L0BU"
      },
      "source": [
        "For step 2, if one would like to finetune from a PTL checkpoint, `init_from_pretrained_model` in config should be replaced with `init_from_nemo_model` and need to provide the path to checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtXUWmYLQ0PJ"
      },
      "outputs": [],
      "source": [
        "test_manifest = os.path.join(data_dir,'an4/wav/an4test_clstk/test.json')\n",
        "finetune_config.model.train_ds.manifest_filepath = test_manifest\n",
        "finetune_config.model.validation_ds.manifest_filepath = test_manifest\n",
        "finetune_config.model.decoder.num_classes = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHy1zE1cTDZn"
      },
      "source": [
        "So we have set up the data and changed the decoder required for finetune, we now just need to create a trainer and start training with a smaller learning rate for fewer epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBmF6tQITSRl"
      },
      "outputs": [],
      "source": [
        "# Setup the new trainer object\n",
        "# Let us modify some trainer configs for this demo\n",
        "# Checks if we have GPU available and uses it\n",
        "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "trainer_config = OmegaConf.create(dict(\n",
        "    devices=1,\n",
        "    accelerator=accelerator,\n",
        "    max_epochs=5,\n",
        "    max_steps=-1,  # computed at runtime if not set\n",
        "    num_nodes=1,\n",
        "    accumulate_grad_batches=1,\n",
        "    enable_checkpointing=False,  # Provided by exp_manager\n",
        "    logger=False,  # Provided by exp_manager\n",
        "    log_every_n_steps=1,  # Interval of logging.\n",
        "    val_check_interval=1.0,  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n",
        "))\n",
        "print(OmegaConf.to_yaml(trainer_config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRz-8-xzUHKZ"
      },
      "outputs": [],
      "source": [
        "trainer_finetune = pl.Trainer(**trainer_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOwHTkW-UUy8"
      },
      "source": [
        "## Setting the trainer to the restored model\n",
        "Setting the trainer to the restored model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FhYQQQOUPIk"
      },
      "outputs": [],
      "source": [
        "log_dir_finetune = exp_manager(trainer_finetune, config.get(\"exp_manager\", None))\n",
        "print(log_dir_finetune)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc3fzGYVVTyi"
      },
      "source": [
        "## Fine-tune training step\n",
        "\n",
        "When fine-tuning on a truly new dataset, we will not see such a dramatic improvement in performance. However, it should still converge a little faster than if it was trained from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6YFF2leL0BV"
      },
      "outputs": [],
      "source": [
        "speaker_model = nemo_asr.models.EncDecSpeakerLabelModel(cfg=finetune_config.model, trainer=trainer_finetune)\n",
        "speaker_model.maybe_init_from_pretrained_checkpoint(finetune_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqyU_4DOL0BV"
      },
      "source": [
        "In the config, we keep weights of preprocessor and encoder, and attach a new decoder as mentioned in above section to match num of classes of new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFIOsuFYVLzr"
      },
      "outputs": [],
      "source": [
        "## Fine-tuning for 5 epochs¶\n",
        "trainer_finetune.fit(speaker_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6lW0ZX4L0BV"
      },
      "source": [
        "Tip: Add more data augmentation and dropout while finetuning on your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DNidtl4VplU"
      },
      "source": [
        "# Saving .nemo file\n",
        "Now we can save the whole config and model parameters in a single .nemo and we can anytime restore from it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am5wej6-VdZW"
      },
      "outputs": [],
      "source": [
        "restored_model.save_to(os.path.join(log_dir_finetune, '..',\"titanet-large-finetune.nemo\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnBhFJefV-Pf"
      },
      "outputs": [],
      "source": [
        "!ls {log_dir_finetune}/.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVx1hNP_V_iz"
      },
      "outputs": [],
      "source": [
        "# restore from a save model\n",
        "restored_model = nemo_asr.models.EncDecSpeakerLabelModel.restore_from(os.path.join(log_dir_finetune, '..', \"titanet-large-finetune.nemo\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80tLWTN40uaB"
      },
      "source": [
        "# Speaker Verification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VciRUIRz0y6P"
      },
      "source": [
        "Training for a speaker verification model is almost the same as the speaker recognition model with a change in the loss function. Angular Loss is a better function to train for a speaker verification model as the model is trained in an end-to-end manner with loss optimizing for embeddings cluster to be far from each other for different speaker by maximizing the angle between these clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULTjBuFI19Js"
      },
      "source": [
        "To train for verification we just need to toggle `angular` flag in `config.model.decoder.params.angular = True` else set it to `False` to train with cross-entropy loss for identification purposes.\n",
        "Once we set this, the loss will be changed to angular loss and we can follow the above steps to the model.\n",
        "Note the scale and margin values to be set for the loss function are present at `config.model.loss.scale` and `config.model.loss.margin`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcKiNEY032-t"
      },
      "source": [
        "## Extract Speaker Embeddings\n",
        "Once you have a trained model or use one of our pretrained nemo checkpoints to get speaker embeddings for any speaker.\n",
        "\n",
        "To demonstrate this we shall use `nemo_asr.models.EncDecSpeakerLabelModel` with say 5 audio_samples from our dev manifest set. This model is specifically for inference purposes to extract embeddings from a trained `.nemo` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXEzKMHf3r6-"
      },
      "outputs": [],
      "source": [
        "verification_model = nemo_asr.models.EncDecSpeakerLabelModel.restore_from(os.path.join(log_dir_finetune, '..', 'titanet-large-finetune.nemo'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-XiLHMQ8BIk"
      },
      "source": [
        "Now, we need to pass the necessary manifest_filepath and params to set up the data loader for extracting embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk2vsDJk9PS8"
      },
      "outputs": [],
      "source": [
        "!head -5 {validation_manifest} > embeddings_manifest.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEd5poCr9yrP"
      },
      "outputs": [],
      "source": [
        "config.model.train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMxs6iBDL0BW"
      },
      "outputs": [],
      "source": [
        "from nemo.collections.asr.parts.utils.speaker_utils import embedding_normalize\n",
        "from tqdm import  tqdm\n",
        "try:\n",
        "    from torch.cuda.amp import autocast\n",
        "except ImportError:\n",
        "    from contextlib import contextmanager\n",
        "\n",
        "    @contextmanager\n",
        "    def autocast(enabled=None):\n",
        "        yield\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle as pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIHok6LD8g0F"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(speaker_model, manifest_file, batch_size=1, embedding_dir='./', device='cuda'):\n",
        "    test_config = OmegaConf.create(\n",
        "        dict(\n",
        "            manifest_filepath=manifest_file,\n",
        "            sample_rate=16000,\n",
        "            labels=None,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            time_length=20,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    speaker_model.setup_test_data(test_config)\n",
        "    speaker_model = speaker_model.to(device)\n",
        "    speaker_model.eval()\n",
        "\n",
        "    all_embs=[]\n",
        "    out_embeddings = {}\n",
        "\n",
        "    for test_batch in tqdm(speaker_model.test_dataloader()):\n",
        "        test_batch = [x.to(device) for x in test_batch]\n",
        "        audio_signal, audio_signal_len, labels, slices = test_batch\n",
        "        with autocast():\n",
        "            _, embs = speaker_model.forward(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
        "            emb_shape = embs.shape[-1]\n",
        "            embs = embs.view(-1, emb_shape)\n",
        "            all_embs.extend(embs.cpu().detach().numpy())\n",
        "        del test_batch\n",
        "\n",
        "    all_embs = np.asarray(all_embs)\n",
        "    all_embs = embedding_normalize(all_embs)\n",
        "    with open(manifest_file, 'r') as manifest:\n",
        "        for i, line in enumerate(manifest.readlines()):\n",
        "            line = line.strip()\n",
        "            dic = json.loads(line)\n",
        "            uniq_name = '@'.join(dic['audio_filepath'].split('/')[-3:])\n",
        "            out_embeddings[uniq_name] = all_embs[i]\n",
        "\n",
        "    embedding_dir = os.path.join(embedding_dir, 'embeddings')\n",
        "    if not os.path.exists(embedding_dir):\n",
        "        os.makedirs(embedding_dir, exist_ok=True)\n",
        "\n",
        "    prefix = manifest_file.split('/')[-1].rsplit('.', 1)[-2]\n",
        "\n",
        "    name = os.path.join(embedding_dir, prefix)\n",
        "    embeddings_file = name + '_embeddings.pkl'\n",
        "    pkl.dump(out_embeddings, open(embeddings_file, 'wb'))\n",
        "    print(\"Saved embedding files to {}\".format(embedding_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2FRecqD-ln5"
      },
      "outputs": [],
      "source": [
        "manifest_filepath = os.path.join(NEMO_ROOT,'embeddings_manifest.json')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "get_embeddings(verification_model, manifest_filepath, batch_size=64,embedding_dir='./', device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfjXPsjzDOgr"
      },
      "source": [
        "Embeddings are stored in dict structure with key-value pair, key being uniq_name generated based on audio_filepath of the sample present in manifest_file in `embedding_dir`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmTeSR6jD28k"
      },
      "outputs": [],
      "source": [
        "ls ./embeddings/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzqd-6vfL0BX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Speaker_Recogniton_Verification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}